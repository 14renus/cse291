{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import os\n",
    "\n",
    "from dataloading import *\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_seq2seq(config, computing_device):\n",
    "    enc = Encoder(config['input_dim'], config['hidden_dim'], config['n_layers'], config['enc']['hid_dropout'], config['enc']['input_dropout'])\n",
    "    dec = Decoder(config['output_dim'], config['hidden_dim'], config['n_layers'], config['dec']['hid_dropout'], config['dec']['input_dropout'])\n",
    "\n",
    "    model = Seq2Seq(enc, dec,computing_device)#.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'],weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_pad_index)\n",
    "    \n",
    "    model = model.to(computing_device)\n",
    "    return model\n",
    "\n",
    "def split_data(filenames_by_type,test_type, train_frac=0.75, BATCH_SIZE=512):\n",
    "    print('...loading data')\n",
    "    if test_type != 'A':\n",
    "        init='A'\n",
    "    else:\n",
    "        init='B'\n",
    "    filename=filenames_by_type[init][0]\n",
    "    q = torch.load(os.path.join(data_dir,filename))\n",
    "    inputs,targets = q[0],q[1]\n",
    "\n",
    "    for typ in filenames_by_type:\n",
    "        if typ==test_type:\n",
    "            continue\n",
    "        if typ==init:\n",
    "            for filename in filenames_by_type[typ][1:]:\n",
    "                q = torch.load(os.path.join(data_dir,filename))\n",
    "                src,trg = q[0],q[1]\n",
    "                inputs=torch.cat([inputs,src],dim=1)\n",
    "                targets=torch.cat([targets,trg],dim=1)\n",
    "        else:\n",
    "            for filename in filenames_by_type[typ]:\n",
    "                q = torch.load(os.path.join(data_dir,filename))\n",
    "                src,trg = q[0],q[1]\n",
    "                inputs=torch.cat([inputs,src],dim=1)\n",
    "                targets=torch.cat([targets,trg],dim=1)\n",
    "    \n",
    "    print(inputs.size())\n",
    "    print(targets.size())\n",
    "    \n",
    "    #shuffle indices\n",
    "    indices = list(range(targets.size()[1]))\n",
    "\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    inputs = inputs[:,indices,:]\n",
    "    targets = targets[:,indices,:]\n",
    "    \n",
    "    print(inputs.size())\n",
    "    print(targets.size())\n",
    "    \n",
    "    # chunk\n",
    "    n_chunks = math.ceil(inputs.size()[1]/BATCH_SIZE)\n",
    "    inputs = torch.chunk(inputs, n_chunks, dim=1) \n",
    "    targets = torch.chunk(targets, n_chunks, dim=1) \n",
    "    \n",
    "    # split train and val\n",
    "    i=int(train_frac*len(inputs))\n",
    "    train_inputs = inputs[:i]\n",
    "    val_inputs = inputs[i:]\n",
    "    \n",
    "    train_targets = targets[:i]\n",
    "    val_targets = targets[i:]\n",
    "    \n",
    "    return train_inputs, train_targets, val_inputs, val_targets\n",
    "    \n",
    "def get_test_data(filenames_by_type,test_type, BATCH_SIZE=512):\n",
    "    filename=filenames_by_type[test_type][0]\n",
    "    q = torch.load(os.path.join(data_dir,filename))\n",
    "    inputs,targets = q[0],q[1]\n",
    "    \n",
    "    for filename in filenames_by_type[test_type][1:]:\n",
    "                q = torch.load(os.path.join(data_dir,filename))\n",
    "                src,trg = q[0],q[1]\n",
    "                inputs=torch.cat([inputs,src],dim=1)\n",
    "                targets=torch.cat([targets,trg],dim=1)   \n",
    "    # chunk\n",
    "    n_chunks = math.ceil(inputs.size()[1]/BATCH_SIZE)\n",
    "    inputs = torch.chunk(inputs, n_chunks, dim=1) \n",
    "    targets = torch.chunk(targets, n_chunks, dim=1) \n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_validate(config,test_type, train_inputs, train_targets, val_inputs, val_targets, N=5):\n",
    "    output_dir='hd={}_nl={}'.format(config['hidden_dim'],config['n_layers':4])\n",
    "    output_file = 'bs={}_lr={}_wd={}_tf={}_hd={}_id={}_fold={}'.format(config['batch_size'],config['learning_rate'],config['weight_decay'],config['teacher_forcing_ratio'],config['enc']['hid_dropout'],config['enc']['input_dropout'],test_type)\n",
    "    output_filepath = os.path.join('output',output_dir,output_file+'.csv')   \n",
    "    \n",
    "    model = init_seq2seq(config, computing_device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'],weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_pad_index)\n",
    "\n",
    "    verbose=config['verbose']\n",
    "    #train_inputs, train_targets, val_inputs, val_targets = split_data(filenames_by_type,test_type, BATCH_SIZE=config['batch_size'])\n",
    "\n",
    "    avg_val_loss=0.0\n",
    "    min_val_loss=100\n",
    "    min_epoch=0\n",
    "    best_state_dict=None\n",
    "    for epoch in range(config['epochs']):\n",
    "        # train \n",
    "        if verbose:\n",
    "            print('...training')\n",
    "        start=time.time()\n",
    "        loss = train(model, train_inputs, train_targets, optimizer, criterion, computing_device,config)\n",
    "        if verbose:\n",
    "            print('   epoch {}: train_loss:{}, time:{}'.format(epoch,loss,time.time()-start))\n",
    "\n",
    "        #validate\n",
    "        if verbose:\n",
    "            print('...validating')\n",
    "        start=time.time()\n",
    "        val_loss = validate(model, val_inputs, val_targets, optimizer, criterion, computing_device)\n",
    "        if verbose:\n",
    "            print('   epoch {}: val_loss:{}, time:{}'.format(epoch,loss,time.time()-start))\n",
    "    \n",
    "        avg_val_loss+=val_loss\n",
    "        \n",
    "        if epoch%N==0:\n",
    "            avg_val_loss/=N\n",
    "            \n",
    "            with open(output_filepath, 'a') as file: \n",
    "                file.write('{}\\n'.format(avg_val_loss))\n",
    "                \n",
    "            # update min, state_dict\n",
    "            if avg_val_loss<min_val_loss:\n",
    "                min_val_loss=avg_val_loss\n",
    "                min_epoch=epoch \n",
    "                best_state_dict = model.state_dict()\n",
    "            # if not decreasing for a while\n",
    "            elif epoch - min_epoch >= config['N_early_stop']:\n",
    "                if best_state_dict:\n",
    "                    PATH = \"./output/{}.pt\".format(output_file)\n",
    "                    torch.save(best_state_dict, PATH)\n",
    "                return min_val_loss\n",
    "                \n",
    "    if best_state_dict:\n",
    "        PATH = \"./output/{}.pt\".format(output_file)\n",
    "        torch.save(best_state_dict, PATH)\n",
    "    \n",
    "    return min_val_loss, min_epoch, config\n",
    "\n",
    "#test_type = 'E'\n",
    "#test_inputs, test_targets = get_test_data(filenames_by_type,test_type, BATCH_SIZE=config['batch_size'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': ['labelled_gen_data1_A'], 'B': ['labelled_extr_data19_B'], 'C': ['labelled_gen_data10_C'], 'D': ['labelled_extr_data2_D'], 'E': ['labelled_extr_data20_E']}\n",
      "...loading data\n",
      "torch.Size([24, 236678, 260])\n",
      "torch.Size([24, 236678, 15])\n",
      "torch.Size([24, 236678, 260])\n",
      "torch.Size([24, 236678, 15])\n",
      "347\n",
      "347\n",
      "116\n",
      "116\n",
      "...training\n",
      "   epoch 0: train_loss:1.737488609226018, time:232.73550868034363\n",
      "...validating\n",
      "   epoch 0: val_loss:1.737488609226018, time:47.52167367935181\n",
      "min val loss: 1.2176886747623312, min epoch: 0\n"
     ]
    }
   ],
   "source": [
    "# try to run one config: train + validate (+ test?)\n",
    "\n",
    "### SET UP ###\n",
    "data_dir = 'data/numerical_data_set_simple_torch'\n",
    "filenames=[]\n",
    "filenames_by_type = {'A':[], 'B':[], 'C':[], 'D':[], 'E':[]}\n",
    "for file in os.listdir(data_dir):\n",
    "    filename, file_extension = os.path.splitext(file)\n",
    "    \n",
    "    typ = filename[-1]\n",
    "    if typ in filenames_by_type:\n",
    "        filenames.append(file)\n",
    "        filenames_by_type[typ].append(file)\n",
    "        \n",
    "## MODIFY JUST FOR TEST, COMMENT OUT FOR REAL RUNS ###\n",
    "for typ in filenames_by_type:\n",
    "    filenames_by_type[typ]=[filenames_by_type[typ][0]]\n",
    "    \n",
    "print(filenames_by_type)\n",
    "\n",
    "computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_digits =10\n",
    "n_chars=256\n",
    "\n",
    "config = {\n",
    "    'epochs':1,\n",
    "    'N_early_stop':10,\n",
    "    'batch_size':512,\n",
    "    'learning_rate':0.001,\n",
    "    'weight_decay':0,\n",
    "    'teacher_forcing_ratio':1.0,\n",
    "    'hidden_dim':512,\n",
    "    'n_layers':4, \n",
    "    'enc': {\n",
    "        'hid_dropout':0.0,\n",
    "        'input_dropout':0.0\n",
    "    },\n",
    "    'dec': {\n",
    "        'hid_dropout':0.0,\n",
    "        'input_dropout':0.0\n",
    "    },\n",
    "    \n",
    "    'input_dim':n_chars+4,\n",
    "    'output_dim':n_digits+5,\n",
    "    \n",
    "    'verbose':True\n",
    "}\n",
    "\n",
    " \n",
    "### GET DATA ###\n",
    "test_type = 'E'\n",
    "train_inputs, train_targets, val_inputs, val_targets = split_data(filenames_by_type,test_type, BATCH_SIZE=config['batch_size'])\n",
    "\n",
    "print(len(train_inputs))\n",
    "print(len(train_targets))\n",
    "print(len(val_inputs))\n",
    "print(len(val_targets))\n",
    "\n",
    "### TRAIN AND VALIDATE ###\n",
    "min_val_loss,min_epoch,curr_config = train_and_validate(config,test_type, train_inputs, train_targets, val_inputs, val_targets, N=5)\n",
    "print('min val loss: {}, min epoch: {}'.format(min_val_loss,min_epoch))\n",
    "\n",
    "min_config=curr_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...training\n",
      "   epoch 0: train_loss:1.702537173496543, time:236.5699977874756\n",
      "...validating\n",
      "   epoch 0: val_loss:1.702537173496543, time:55.62280201911926\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Seq2Seq' object has no attribute 'stat_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4229ea472b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### TRAIN AND VALIDATE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmin_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcurr_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'min val loss: {}, min epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_val_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmin_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmin_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-eb2bd8beed15>\u001b[0m in \u001b[0;36mtrain_and_validate\u001b[0;34m(config, test_type, train_inputs, train_targets, val_inputs, val_targets, N)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mmin_val_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mavg_val_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mmin_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mbest_state_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;31m# if not decreasing for a while\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmin_epoch\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'N_early_stop'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    530\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 532\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Seq2Seq' object has no attribute 'stat_dict'"
     ]
    }
   ],
   "source": [
    "### TRAIN AND VALIDATE ###\n",
    "min_val_loss,min_epoch,curr_config = train_and_validate(config,test_type, train_inputs, train_targets, val_inputs, val_targets, N=5)\n",
    "print('min val loss: {}, min epoch: {}'.format(min_val_loss,min_epoch))\n",
    "\n",
    "min_config=curr_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min test loss: 5.904045765216534\n"
     ]
    }
   ],
   "source": [
    "### TEST ON BEST MODEL ###\n",
    "test_inputs, test_targets = get_test_data(filenames_by_type,test_type, BATCH_SIZE=config['batch_size'])  \n",
    "model = init_seq2seq(min_config, computing_device)\n",
    "output_dir='hd={}_nl={}'.format(config['hidden_dim'],config['n_layers':4])\n",
    "file = 'bs={}_lr={}_wd={}_tf={}_hd={}_id={}_fold={}'.format(config['batch_size'],config['learning_rate'],config['weight_decay'],config['teacher_forcing_ratio'],config['enc']['hid_dropout'],config['enc']['input_dropout'],test_type)\n",
    "PATH = \"./output/{}/{}.pt\".format(output_dir,file)\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "optimizer = optim.Adam(model.parameters(), lr=min_config['learning_rate'],weight_decay=min_config['weight_decay'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=output_pad_index)\n",
    "test_loss = validate(model, test_inputs, test_targets, optimizer, criterion, computing_device)\n",
    "\n",
    "if config['verbose']:\n",
    "    print('min test loss: {}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
