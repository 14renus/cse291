{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from dataloading import *\n",
    "from model import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_digits =10\n",
    "n_chars=256\n",
    "\n",
    "config = {\n",
    "    'batch_size':512,\n",
    "    'learning_rate':0.001,\n",
    "    'weight_decay':0,\n",
    "    'teacher_forcing_ratio':1.0,\n",
    "    'hidden_dim':512,\n",
    "    'n_layers':4, \n",
    "    'enc': {\n",
    "        'hid_dropout':0.0,\n",
    "        'input_dropout':0.0\n",
    "    },\n",
    "    'dec': {\n",
    "        'hid_dropout':0.0,\n",
    "        'input_dropout':0.0\n",
    "    },\n",
    "    \n",
    "    'input_dim':n_chars+4,\n",
    "    'output_dim':n_digits+5\n",
    "}\n",
    "\n",
    "def init_seq2seq(config, computing_device):\n",
    "    enc = Encoder(config['input_dim'], config['hidden_dim'], config['n_layers'], config['enc']['hid_dropout'], config['enc']['input_dropout'])\n",
    "    dec = Decoder(config['output_dim'], config['hidden_dim'], config['n_layers'], config['dec']['hid_dropout'], config['dec']['input_dropout'])\n",
    "\n",
    "    model = Seq2Seq(enc, dec,computing_device)#.to(device)\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'],weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_pad_index)\n",
    "    \n",
    "    model = model.to(computing_device)\n",
    "    return model\n",
    "\n",
    "def split_data(filenames_by_type,test_type, train_frac=0.75, BATCH_SIZE=512):\n",
    "    print('...loading data')\n",
    "    if test_type != 'A':\n",
    "        init='A'\n",
    "    else:\n",
    "        init='B'\n",
    "    filename=filenames_by_type[init][0]\n",
    "    q = torch.load(os.path.join(data_dir,filename))\n",
    "    inputs,targets = q[0],q[1]\n",
    "\n",
    "    for typ in filenames_by_type:\n",
    "        if typ==test_type:\n",
    "            continue\n",
    "        if typ==init:\n",
    "            for filename in filenames_by_type[typ][1:]:\n",
    "                q = torch.load(os.path.join(data_dir,filename))\n",
    "                src,trg = q[0],q[1]\n",
    "                inputs=torch.cat([inputs,src],dim=1)\n",
    "                targets=torch.cat([targets,trg],dim=1)\n",
    "        else:\n",
    "            for filename in filenames_by_type[typ]:\n",
    "                q = torch.load(os.path.join(data_dir,filename))\n",
    "                src,trg = q[0],q[1]\n",
    "                inputs=torch.cat([inputs,src],dim=1)\n",
    "                targets=torch.cat([targets,trg],dim=1)\n",
    "    \n",
    "    #shuffle indices\n",
    "    indices = list(range(len(targets)))\n",
    "    random.shuffle(indices)\n",
    "    \n",
    "    inputs = inputs[:,indices,:]\n",
    "    targets = targets[:,indices,:]\n",
    "    \n",
    "    # chunk\n",
    "    n_chunks = math.ceil(inputs.size()[1]/BATCH_SIZE)\n",
    "    inputs = torch.chunk(inputs, n_chunks, dim=1) \n",
    "    targets = torch.chunk(targets, n_chunks, dim=1) \n",
    "    \n",
    "    # split train and val\n",
    "    train_inputs = inputs[:train_frac*len(train_inputs)]\n",
    "    val_inputs = inputs[train_frac*len(train_inputs):]\n",
    "    \n",
    "    train_targets = targets[:train_frac*len(train_inputs)]\n",
    "    val_targets = targets[train_frac*len(train_inputs):]\n",
    "    \n",
    "    return train_inputs, train_targets, val_inputs, val_targets\n",
    "    \n",
    "def get_test_data(filenames_by_type,test_type, BATCH_SIZE=512):\n",
    "    filename=filenames_by_type[test_type][0]\n",
    "    q = torch.load(os.path.join(data_dir,filename))\n",
    "    inputs,targets = q[0],q[1]\n",
    "    \n",
    "    for filename in filenames_by_type[test_type][1:]:\n",
    "                q = torch.load(os.path.join(data_dir,filename))\n",
    "                src,trg = q[0],q[1]\n",
    "                inputs=torch.cat([inputs,src],dim=1)\n",
    "                targets=torch.cat([targets,trg],dim=1)   \n",
    "    # chunk\n",
    "    n_chunks = math.ceil(inputs.size()[1]/BATCH_SIZE)\n",
    "    inputs = torch.chunk(inputs, n_chunks, dim=1) \n",
    "    targets = torch.chunk(targets, n_chunks, dim=1) \n",
    "    \n",
    "    return inputs, targets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "{'A': ['labelled_gen_data1_A', 'labelled_gen_data3_A', 'labelled_gen_data2_A', 'labelled_extr_data1_A', 'labelled_extr_data3_A'], 'B': ['labelled_extr_data19_B', 'labelled_extr_data18_B', 'labelled_gen_data5_B', 'labelled_gen_data4_B', 'labelled_gen_data6_B'], 'C': ['labelled_gen_data10_C', 'labelled_gen_data11_C', 'labelled_gen_data7_C', 'labelled_gen_data8_C', 'labelled_gen_data9_C'], 'D': ['labelled_extr_data2_D', 'labelled_gen_data12_D', 'labelled_gen_data13_D', 'labelled_extr_data4_D'], 'E': ['labelled_extr_data20_E', 'labelled_gen_data16_E', 'labelled_dir_data91_E', 'labelled_gen_data15_E', 'labelled_dir_data92_E', 'labelled_gen_data14_E', 'labelled_dir_data39_E', 'labelled_dir_data49_E']}\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = 'data/numerical_data_set_simple_torch'\n",
    "\n",
    "filenames = []\n",
    "filenames_by_type = {'A':[], 'B':[], 'C':[], 'D':[], 'E':[]}\n",
    "for file in os.listdir(data_dir):\n",
    "    filename, file_extension = os.path.splitext(file)\n",
    "    \n",
    "    typ = filename[-1]\n",
    "    if typ in filenames_by_type:\n",
    "        filenames.append(file)\n",
    "        filenames_by_type[typ].append(file)\n",
    "        \n",
    "print(len(filenames))\n",
    "print(filenames_by_type)\n",
    "for key in filenames_by_type:\n",
    "    print(len(filenames_by_type[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...loading data\n"
     ]
    }
   ],
   "source": [
    "computing_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_digits =10\n",
    "n_chars=256\n",
    "\n",
    "config = {\n",
    "    'epochs':100,\n",
    "    'batch_size':512,\n",
    "    'learning_rate':0.001,\n",
    "    'weight_decay':0,\n",
    "    'teacher_forcing_ratio':1.0,\n",
    "    'hidden_dim':512,\n",
    "    'n_layers':4, \n",
    "    'enc': {\n",
    "        'hid_dropout':0.0,\n",
    "        'input_dropout':0.0\n",
    "    },\n",
    "    'dec': {\n",
    "        'hid_dropout':0.0,\n",
    "        'input_dropout':0.0\n",
    "    },\n",
    "    \n",
    "    'input_dim':n_chars+4,\n",
    "    'output_dim':n_digits+5\n",
    "}\n",
    "\n",
    "def train_and_validate(config,test_type, train_inputs, train_targets, val_inputs, val_targets):\n",
    "    model = init_seq2seq(config, computing_device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'],weight_decay=config['weight_decay'])\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=output_pad_index)\n",
    "\n",
    "    train_inputs, train_targets, val_inputs, val_targets = split_data(filenames_by_type,test_type, BATCH_SIZE=config['batch_size'])\n",
    "\n",
    "    # train \n",
    "    print('...training')\n",
    "    for epoch in range(num_epochs):\n",
    "        start=time.time()\n",
    "        loss = train(model, inputs, targets, optimizer, criterion, computing_device,config)\n",
    "        print('   epoch {}: train_loss:{}, time:{}'.format(epoch,loss,time.time()-start))\n",
    "\n",
    "    #validate\n",
    "    print('...validating')\n",
    "    start=time.time()\n",
    "    val_loss = evaluate(model, val_inputs, val_targets, optimizer, criterion, computing_device)\n",
    "    print('   epoch {}: val_loss:{}, time:{}'.format(epoch,loss,time.time()-start))\n",
    "\n",
    "\n",
    "test_type = 'E'\n",
    "test_inputs, test_targets = get_test_data(filenames_by_type,test_type, BATCH_SIZE=config['batch_size'])  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
